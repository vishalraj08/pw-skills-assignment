{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Classification: Decision Trees, SVM, and Naive Bayes\n",
    "**Assignment** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: What is Information Gain, and how is it used in Decision Trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "**Information Gain** is a metric used to measure the reduction in entropy (or uncertainty) achieved by splitting a dataset based on a specific attribute. It calculates the difference between the entropy of the parent node and the weighted average entropy of the child nodes.\n",
    "\n",
    "**How it is used in Decision Trees:**\n",
    "1.  **Selection Criteria:** Decision Trees use Information Gain to decide which feature to split on at each step.\n",
    "2.  **Highest Gain:** The algorithm calculates the Information Gain for every possible feature. The feature with the highest Information Gain is chosen as the root (or split) node because it provides the best separation of the data into pure classes.\n",
    "3.  **Process:** This process is repeated recursively for each child node until the tree is fully grown or a stopping criterion is met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: What is the difference between Gini Impurity and Entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Both Gini Impurity and Entropy are metrics used to measure the \"impurity\" or disorder of a node in a Decision Tree, but they differ in their mathematical formulation and computational properties.\n",
    "\n",
    "| Feature | Gini Impurity | Entropy |\n",
    "| :--- | :--- | :--- |\n",
    "| **Formula** | $1 - \\sum (p_i)^2$ | $- \\sum p_i \\log_2(p_i)$ |\n",
    "| **Range** | 0 to 0.5 (for binary classification) | 0 to 1 (for binary classification) |\n",
    "| **Computation** | Computationally faster (uses simple squaring). | Computationally more expensive (uses logarithmic calculations). |\n",
    "| **Sensitivity** | Biased towards finding the majority class. | Slightly more balanced, penalizes impurity more heavily. |\n",
    "| **Use Case** | Default in libraries like Scikit-Learn (CART algorithm). | Used in algorithms like ID3 and C4.5. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: What is Pre-Pruning in Decision Trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "**Pre-Pruning** (also known as Early Stopping) is the process of halting the growth of a Decision Tree before it perfectly classifies the training set. \n",
    "\n",
    "Instead of allowing the tree to grow until every leaf is pure (which often leads to overfitting), constraints are applied during the training process.\n",
    "\n",
    "**Common Pre-Pruning Hyperparameters:**\n",
    "* **Max Depth:** Limiting the maximum height of the tree.\n",
    "* **Min Samples Split:** Setting a minimum number of samples required to split an internal node.\n",
    "* **Min Samples Leaf:** Setting a minimum number of samples required to be at a leaf node.\n",
    "* **Max Leaf Nodes:** Limiting the total number of leaf nodes in the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances:\n",
      "sepal length (cm): 0.0000\n",
      "sepal width (cm): 0.0167\n",
      "petal length (cm): 0.9061\n",
      "petal width (cm): 0.0772\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 1. Load the dataset (Using Iris dataset as a standard example)\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# 2. Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Initialize the Decision Tree Classifier with Gini Impurity\n",
    "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "\n",
    "# 4. Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 5. Print Feature Importances\n",
    "print(\"Feature Importances:\")\n",
    "for name, importance in zip(data.feature_names, clf.feature_importances_):\n",
    "    print(f\"{name}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: What is a Support Vector Machine (SVM)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "A **Support Vector Machine (SVM)** is a powerful supervised learning algorithm used for both classification and regression tasks. \n",
    "\n",
    "**Key Concepts:**\n",
    "* **Hyperplane:** SVM finds the optimal boundary (hyperplane) that best separates the data points of different classes.\n",
    "* **Margin:** It aims to maximize the \"margin,\" which is the distance between the hyperplane and the nearest data points from either class.\n",
    "* **Support Vectors:** The data points closest to the hyperplane that influence its position and orientation are called support vectors.\n",
    "* **Goal:** By maximizing the margin, SVM tries to improve the model's generalization ability to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: What is the Kernel Trick in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "The **Kernel Trick** is a technique used in SVM to handle non-linearly separable data. \n",
    "\n",
    "**How it works:**\n",
    "1.  **Transformation:** It implicitly maps the input data from a lower-dimensional space (where it is not linearly separable) into a higher-dimensional space.\n",
    "2.  **Linear Separation:** In this higher-dimensional space, the data becomes linearly separable by a hyperplane.\n",
    "3.  **Efficiency:** It computes the dot product of the data points in the high-dimensional space without actually performing the complex transformation explicitly. This makes the computation highly efficient.\n",
    "\n",
    "**Common Kernels:** Linear, Polynomial, Radial Basis Function (RBF), and Sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7: Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Linear Kernel: 0.9815\n",
      "Accuracy with RBF Kernel:    0.7593\n",
      "\n",
      "Conclusion: The Linear kernel performed better on this split.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "# 1. Load the Wine dataset\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# 2. Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 3. Train SVM with Linear Kernel\n",
    "svc_linear = SVC(kernel='linear', random_state=42)\n",
    "svc_linear.fit(X_train, y_train)\n",
    "y_pred_linear = svc_linear.predict(X_test)\n",
    "acc_linear = accuracy_score(y_test, y_pred_linear)\n",
    "\n",
    "# 4. Train SVM with RBF Kernel\n",
    "svc_rbf = SVC(kernel='rbf', random_state=42)\n",
    "svc_rbf.fit(X_train, y_train)\n",
    "y_pred_rbf = svc_rbf.predict(X_test)\n",
    "acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
    "\n",
    "# 5. Compare Accuracies\n",
    "print(f\"Accuracy with Linear Kernel: {acc_linear:.4f}\")\n",
    "print(f\"Accuracy with RBF Kernel:    {acc_rbf:.4f}\")\n",
    "\n",
    "if acc_linear > acc_rbf:\n",
    "    print(\"\\nConclusion: The Linear kernel performed better on this split.\")\n",
    "elif acc_rbf > acc_linear:\n",
    "    print(\"\\nConclusion: The RBF kernel performed better on this split.\")\n",
    "else:\n",
    "    print(\"\\nConclusion: Both kernels performed equally well.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "**Naïve Bayes Classifier** is a probabilistic machine learning model based on **Bayes' Theorem**. It is used for classification tasks (like spam filtering and sentiment analysis) and predicts the probability that a given data point belongs to a particular class.\n",
    "\n",
    "**Why it is called \"Naïve\":**\n",
    "* It makes a \"naïve\" (simplistic) assumption that all features in the dataset are **mutually independent**.\n",
    "* For example, if a fruit is described as \"Red\", \"Round\", and \"Diameter of 3 inches\", Naïve Bayes assumes these features contribute independently to the probability of it being an apple, even though in reality, these features might depend on each other.\n",
    "* Despite this strong and often unrealistic assumption, the classifier performs remarkably well in many real-world situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "These are variations of the Naïve Bayes algorithm tailored for different types of data distributions:\n",
    "\n",
    "1.  **Gaussian Naïve Bayes:**\n",
    "    * **Data Type:** Used when features are **continuous** (real-valued numbers).\n",
    "    * **Assumption:** It assumes that the continuous values associated with each class are distributed according to a Gaussian (Normal) distribution (Bell curve).\n",
    "    * **Example:** Predicting Iris flower species based on sepal length (cm).\n",
    "\n",
    "2.  **Multinomial Naïve Bayes:**\n",
    "    * **Data Type:** Used for **discrete counts**.\n",
    "    * **Assumption:** It assumes the data follows a Multinomial distribution.\n",
    "    * **Example:** Text classification (Spam vs. Ham), where features are the frequency counts of words in a document.\n",
    "\n",
    "3.  **Bernoulli Naïve Bayes:**\n",
    "    * **Data Type:** Used for **binary/boolean** features.\n",
    "    * **Assumption:** It assumes the data follows a Multivariate Bernoulli distribution.\n",
    "    * **Example:** Text classification where we only care if a word is present (1) or absent (0), rather than counting how many times it appears."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10: Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Gaussian Naïve Bayes: 0.9737\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# 2. Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Initialize Gaussian Naïve Bayes Classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# 4. Train the model\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# 5. Make predictions\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# 6. Evaluate Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of Gaussian Naïve Bayes: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303173b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
